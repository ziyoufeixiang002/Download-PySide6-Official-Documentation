name: Crawl Kotlin Documentation - Parallel Download

on:
  workflow_dispatch:

jobs:
  crawl-kotlin-docs:
    runs-on: ubuntu-latest
    timeout-minutes: 330  # 5.5小时超时保护
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Install tools for parallel download
      run: |
        sudo apt-get update
        sudo apt-get install -y wget python3 python3-pip aria2 parallel
        pip3 install requests beautifulsoup4 lxml
      
    - name: Create directories
      run: |
        mkdir -p kotlin-docs
        mkdir -p partial-downloads
        mkdir -p url-tracking
        
    - name: Generate URL list first
      run: |
        # 使用Python脚本先获取所有要下载的URL
        cat > generate_urls.py << 'EOF'
        import requests
        from bs4 import BeautifulSoup
        import urllib.parse
        import sys
        
        def get_all_links(base_url, max_level=3, current_level=0, visited=None):
            if visited is None:
                visited = set()
                
            if current_level > max_level or base_url in visited:
                return set()
                
            visited.add(base_url)
            all_links = set()
            
            try:
                response = requests.get(base_url, timeout=10)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    full_url = urllib.parse.urljoin(base_url, href)
                    
                    # 只保留docs和api开头的链接
                    if 'kotlinlang.org/docs' in full_url or 'kotlinlang.org/api' in full_url:
                        # 过滤掉媒体文件
                        if not any(ext in full_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.mp3', '.wav', '.ogg', '.mp4', '.avi', '.mov', '.flv', '.webm', '.pdf', '.js', '.css', '.zip']):
                            if full_url not in visited:
                                all_links.add(full_url)
                                print(f"Found: {full_url}")
                
                # 递归获取下一级链接
                next_level_links = set()
                for link in list(all_links):
                    next_level_links |= get_all_links(link, max_level, current_level + 1, visited)
                
                all_links |= next_level_links
                
            except Exception as e:
                print(f"Error processing {base_url}: {e}")
                
            return all_links
        
        # 主程序
        docs_links = get_all_links('https://kotlinlang.org/docs/')
        api_links = get_all_links('https://kotlinlang.org/api/')
        
        all_unique_links = docs_links | api_links
        
        print(f"Total unique URLs found: {len(all_unique_links)}")
        
        with open('url_list.txt', 'w') as f:
            for url in sorted(all_unique_links):
                f.write(url + '\n')
        EOF
        
        python3 generate_urls.py
        
        echo "URL列表生成完成，共找到 $(wc -l < url_list.txt) 个唯一链接"
        head -10 url_list.txt
      
    - name: Download using aria2 for parallel downloads
      run: |
        cd kotlin-docs
        
        # 使用aria2进行并行下载，大幅提高速度
        # -j 10: 同时下载10个文件
        # -x 10: 每个文件使用10个连接
        # -s 10: 将文件分成10个片段并行下载
        timeout 4h aria2c \
          -i ../url_list.txt \
          -j 10 \
          -x 10 \
          -s 10 \
          --timeout=30 \
          --max-tries=3 \
          --retry-wait=1 \
          --user-agent="Mozilla/5.0 (compatible; Docs-Crawler)" \
          --file-allocation=none \
          --continue=true \
          --max-connection-per-server=10 \
          --split=10 \
          --min-split-size=1M \
          --stream-piece-selector=random \
          --enable-http-keep-alive=true \
          --enable-http-pipelining=true \
          --check-certificate=false \
          --auto-file-renaming=false \
          --dir=. \
          --out="file.%(ext)s" || true
          
        echo "aria2c下载进程结束"
        
    - name: Alternative parallel download with wget (if aria2 fails)
      if: failure()
      run: |
        cd kotlin-docs
        
        # 如果aria2失败，使用GNU parallel配合wget进行并行下载
        # 分割URL列表为多个部分
        split -l 100 ../url_list.txt url_chunk_
        
        # 使用parallel并行下载，最多同时5个进程
        find . -name "url_chunk_*" | parallel -j 5 \
          "wget --input-file={} \
           --no-check-certificate \
           --timeout=20 \
           --tries=2 \
           --wait=0 \
           --html-extension \
           --convert-links \
           --restrict-file-names=unix \
           --user-agent='Mozilla/5.0 (compatible; Docs-Crawler)' \
           --progress=bar \
           --timestamping \
           --level=0 \
           --recursive || echo '部分下载失败: {}'"
        
        # 清理临时文件
        rm -f url_chunk_*
        
        echo "并行wget下载完成"
        
    - name: Organize downloaded files
      run: |
        # 备份已下载内容
        if [ "$(ls -A kotlin-docs 2>/dev/null)" ]; then
          echo "备份已下载的 $(find kotlin-docs -type f | wc -l) 个文件..."
          cp -r kotlin-docs/* partial-downloads/ 2>/dev/null || true
        fi
        
        # 重命名文件，确保有正确的扩展名
        find kotlin-docs -type f -name "file.html*" -exec bash -c '
          for file; do
            # 提取原始URL信息并重命名文件
            newname=$(echo "$file" | sed "s/file.html\.//" | sed "s/\.\?[0-9]*$//")
            if [ -f "$file" ] && [ ! -f "$newname" ]; then
              mv "$file" "$newname" 2>/dev/null || true
            fi
          done
        ' bash {} +
        
        # 清理小文件和空文件
        find kotlin-docs -size -100c -delete 2>/dev/null || true
        find kotlin-docs -type d -empty -delete 2>/dev/null || true
        
    - name: Remove duplicates and generate report
      run: |
        echo "开始处理重复文件..."
        
        # 基于内容去重
        find kotlin-docs -type f -name "*.html" -exec md5sum {} \; | \
          sort | uniq -w32 -d | while read md5 filename; do
          echo "删除重复文件: $filename"
          rm "$filename"
        done 2>/dev/null || true
        
        # 生成最终报告
        echo "=== 最终下载报告 ===" > kotlin-docs/download-report.txt
        echo "唯一URL总数: $(wc -l < url_list.txt)" >> kotlin-docs/download-report.txt
        echo "最终HTML文件数: $(find kotlin-docs -name "*.html" | wc -l)" >> kotlin-docs/download-report.txt
        echo "总文件大小: $(du -sh kotlin-docs | cut -f1)" >> kotlin-docs/download-report.txt
        echo "下载完成时间: $(date)" >> kotlin-docs/download-report.txt
        
        echo "文档分类统计:"
        echo "- docs 部分: $(find kotlin-docs -path "*/docs/*" -name "*.html" 2>/dev/null | wc -l)"
        echo "- api 部分: $(find kotlin-docs -path "*/api/*" -name "*.html" 2>/dev/null | wc -l)"
        
        # 生成站点地图
        find kotlin-docs -name "*.html" | sed 's|kotlin-docs/||' | sort > kotlin-docs/sitemap.txt 2>/dev/null || true
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: kotlin-docs-parallel-$(date +%s)
        path: |
          kotlin-docs/
          url_list.txt
        retention-days: 7
        
    - name: Final status
      run: |
        total_urls=$(wc -l < url_list.txt)
        downloaded_files=$(find kotlin-docs -name "*.html" | wc -l)
        success_rate=$((downloaded_files * 100 / total_urls))
        
        echo "=== 下载完成 ==="
        echo "URL总数: $total_urls"
        echo "下载文件数: $downloaded_files"
        echo "成功率: $success_rate%"
        
        if [ $success_rate -gt 80 ]; then
          echo "✅ 下载成功！"
        elif [ $success_rate -gt 50 ]; then
          echo "⚠️  部分下载成功"
        else
          echo "❌ 下载完成但成功率较低"
        fi
